## Overview

The `generate_model_response.py` script is designed to generate responses for **Predicate Label (PL)** and **Research Field (RF)** tasks using transformer-based language models. This script supports:

- **Model Types**: MoE (Mixture of Experts), FT (Fine-Tuned), and Base models.
- **Prompt Types**: Zero-shot, Few-shot, Chain-of-Thought (CoT), and Zero-shot CoT.
- **Task Types**: Predicate Label (PL) and Research Field (RF).

---

## Features
- Customizable prompts for each task and model type.
- Retry mechanism for robust response generation.
- Efficient text generation using Hugging Face Transformers pipeline.
- Outputs raw responses and extracted results in structured format.

---

## Requirements

### Python Libraries
- `torch`
- `transformers`
- `pandas`
- `tqdm`

### Installation
Install the required Python libraries:
```bash
pip install torch transformers pandas tqdm
```

---

## Usage

Run the script using the following command:
```bash
python generate_model_response.py \
  --model_type <MODEL_TYPE> \
  --task <TASK_TYPE> \
  --model_id <MODEL_ID> \
  --data_file <DATA_FILE> \
  --output_dir <OUTPUT_DIR> \
  --max_retries <MAX_RETRIES> \
  --max_tokens <MAX_TOKENS> \
  --prompt_type <PROMPT_TYPE> \
  --version <VERSION>
```

### Arguments
- `--model_type` (required): Type of model. Options are:
  - `MoE` (Mixture of Experts)
  - `FT` (Fine-Tuned)
  - `Base`
- `--task` (required): Task type. Options are:
  - `pl` (Predicate Label)
  - `rf` (Research Field)
- `--model_id` (required): Hugging Face model ID to load.
- `--data_file` (required): Path to the input data file in pickle format.
- `--output_dir` (required): Directory to save the output responses.
- `--max_retries` (optional): Maximum number of retries for failed generations (default: `3`).
- `--max_tokens` (optional): Maximum number of tokens to generate (default: `512`).
- `--prompt_type` (optional): Type of prompt. Options are:
  - `zero_shot`
  - `few_shot`
  - `cot` (Chain-of-Thought)
  - `zero_shot_cot`
- `--version` (optional): Version identifier for the output (default: `v1`).

---

### Example Command

```bash
python generate_model_response.py \
  --model_type FT \
  --task pl \
  --model_id meta-llama/Llama-3.1-8B \
  --data_file /path/to/input.pkl \
  --output_dir /path/to/output/ \
  --max_retries 5 \
  --max_tokens 512 \
  --prompt_type cot \
  --version v2
```

---

## Input Data Format
The input data file must be in Pickle (`.pkl`) format and contain the following columns:
- `title`: The title of the research paper.
- `abstract`: The abstract of the research paper.

---

## Output
The script saves the generated responses in a structured pickle file in the specified `output_dir`. The file contains the following columns:
- `title`: Title of the research paper.
- `raw_response`: The raw output text generated by the model.
- `extracted`: The extracted relevant data (Predicate Labels or Research Field).

### Output File Example
For `output_dir=/path/to/output/`, the file will be saved as:
```
/path/to/output/<MODEL_TYPE>_<TASK_TYPE>_<VERSION>_responses.pkl
```

---

## Prompt Types
### Zero-shot
- Generates responses without prior examples.
### Few-shot
- Includes a few examples in the prompt to guide the model.
### Chain-of-Thought (CoT)
- Guides the model to reason step-by-step before generating the final output.
### Zero-shot CoT
- Combines zero-shot learning with chain-of-thought reasoning.

---

## Notes
- Ensure the Hugging Face model ID and input data file path are valid.
- Adjust the `max_retries` and `max_tokens` values based on your requirements and available compute resources.

---

## License
This script is provided for educational and research purposes. Feel free to modify and adapt it to your needs.

